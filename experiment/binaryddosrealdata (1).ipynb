{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4059918,"sourceType":"datasetVersion","datasetId":2398189},{"sourceId":10583425,"sourceType":"datasetVersion","datasetId":6549500},{"sourceId":11551543,"sourceType":"datasetVersion","datasetId":7243943}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyTsetlinMachine pandas scikit-learn pyarrow matplotlib seaborn\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom pyTsetlinMachine.tm import MultiClassTsetlinMachine\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-07T03:33:43.125939Z","iopub.execute_input":"2025-06-07T03:33:43.126121Z","iopub.status.idle":"2025-06-07T03:33:52.632131Z","shell.execute_reply.started":"2025-06-07T03:33:43.126104Z","shell.execute_reply":"2025-06-07T03:33:52.631104Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pyTsetlinMachine\n  Downloading pyTsetlinMachine-0.6.6.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (19.0.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nBuilding wheels for collected packages: pyTsetlinMachine\n  Building wheel for pyTsetlinMachine (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyTsetlinMachine: filename=pyTsetlinMachine-0.6.6-cp311-cp311-linux_x86_64.whl size=59490 sha256=3f31c163e52f00ebec10ed5acb21686ee87e3221aa2204aad7fe0ad143d83307\n  Stored in directory: /root/.cache/pip/wheels/46/fb/7b/94130662b0133acfaf27f087dcaa857e5280c61190f90de342\nSuccessfully built pyTsetlinMachine\nInstalling collected packages: pyTsetlinMachine\nSuccessfully installed pyTsetlinMachine-0.6.6\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"This version of the script uses binarization strategies for features","metadata":{}},{"cell_type":"markdown","source":"Source of training dataset:\n\nhttps://ieeexplore.ieee.org/abstract/document/8888419\n\nhttps://www.kaggle.com/datasets/dhoogla/cicddos2019\n\n\n(for preliminary testing, the test-train split in this dataset has been used)","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport joblib\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\ndata_dir = \"/kaggle/input/myddosdata\"\nattack_data = pd.read_json(os.path.join(data_dir, \"attack.json\"), lines=True)\nbenign_train = pd.read_json(os.path.join(data_dir, \"Benign_Train.json\"), lines=True)\nbenign_test = pd.read_json(os.path.join(data_dir, \"Benign_Test.json\"), lines=True)\n\nattack_data['Label'] = 1\nbenign_train['Label'] = 0\nbenign_test['Label'] = 0\n\nattack_train = attack_data.sample(n=900, random_state=42)\nattack_test = attack_data.drop(attack_train.index)\n\nattack_test.to_json(\"attack_test_samples.json\", orient=\"records\", lines=True)\n\ntrain_data = pd.concat([benign_train, attack_train], ignore_index=True)\ntest_data = pd.concat([benign_test, attack_test], ignore_index=True)\n\ndef rename_columns(df):\n    return df.rename(columns=lambda x: x.replace('_', ' ').title() if isinstance(x, str) else x)\n\ntrain_data = rename_columns(train_data)\ntest_data = rename_columns(test_data)\n\ncategorical_features = ['Protocol']\nprotocol_categories = [[0, 6, 17]]\none_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', categories=protocol_categories)\n\nencoded_train = one_hot_encoder.fit_transform(train_data[categorical_features])\nencoded_test = one_hot_encoder.transform(test_data[categorical_features])\njoblib.dump(one_hot_encoder, \"onehot_encoder.pkl\")\n\ntrain_encoded_df = pd.DataFrame(encoded_train, columns=one_hot_encoder.get_feature_names_out(categorical_features), index=train_data.index)\ntest_encoded_df = pd.DataFrame(encoded_test, columns=one_hot_encoder.get_feature_names_out(categorical_features), index=test_data.index)\n\ntrain_data = pd.concat([train_data.drop(columns=categorical_features), train_encoded_df], axis=1)\ntest_data = pd.concat([test_data.drop(columns=categorical_features), test_encoded_df], axis=1)\n\nbinary_flags = [\"Fwd Psh Flags\", \"Bwd Psh Flags\", \"Fwd Urg Flags\", \"Bwd Urg Flags\"]\nfor flag in binary_flags:\n    if flag in train_data.columns:\n        train_data[flag] = (train_data[flag] > 0).astype(int)\n        test_data[flag] = (test_data[flag] > 0).astype(int)\n\nfinal_features = [\n    \"Flow Iat Mean\", \"Idle Mean\", \"Fwd Iat Mean\", \"Packet Length Mean\", \"Fwd Packet Length Mean\",\n    \"Flow Iat Std\", \"Fwd Packet Length Min\", \"Idle Min\", \"Flow Iat Min\", \"Init Fwd Win Bytes\",\n    \"Packet Length Variance\", \"Cwe Flag Count\", \"Protocol_0\", \"Protocol_6\", \"Protocol_17\",\n    \"Flow Packets Per S\", \"Fwd Packets Per S\", \"Fwd Psh Flags\", \"Fwd Act Data Packets\", \"Fwd Iat Std\",\n    \"Avg Fwd Segment Size\", \"Flow Iat Max\", \"Total Fwd Packets\", \"Subflow Fwd Packets\",\n    \"Fwd Iat Min\", \"Urg Flag Count\", \"Ack Flag Count\", \"Rst Flag Count\", \"Fwd Packet Length Std\",\n    \"Fwd Iat Max\", \"Packet Length Min\", \"Active Max\", \"Label\"\n]\n\ntrain_data = train_data[[f for f in final_features if f in train_data.columns]]\ntest_data = test_data[[f for f in final_features if f in test_data.columns]]\n\nX_train = train_data.drop(columns=['Label'])\ny_train = train_data['Label']\nX_test = test_data.drop(columns=['Label'])\ny_test = test_data['Label']\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\njoblib.dump(scaler, \"minmax_scaler.pkl\")\n\nprint(\" Preprocessing complete!\")\nprint(f\" Train shape: {X_train.shape}\")\nprint(f\" Test shape: {X_test.shape}\")\n\nprint(f\"Final number of features used for training: {X_train.shape[1]}\")\nprint(\"Features:\")\nfor feature in train_data.drop(columns=['Label']).columns:\n    print(f\"- {feature}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:34:59.222177Z","iopub.execute_input":"2025-06-07T03:34:59.222562Z","iopub.status.idle":"2025-06-07T03:34:59.412986Z","shell.execute_reply.started":"2025-06-07T03:34:59.222534Z","shell.execute_reply":"2025-06-07T03:34:59.412079Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Preprocessing complete!\n Train shape: (2019, 32)\n Test shape: (237, 32)\nFinal number of features used for training: 32\nFeatures:\n- Flow Iat Mean\n- Idle Mean\n- Fwd Iat Mean\n- Packet Length Mean\n- Fwd Packet Length Mean\n- Flow Iat Std\n- Fwd Packet Length Min\n- Idle Min\n- Flow Iat Min\n- Init Fwd Win Bytes\n- Packet Length Variance\n- Cwe Flag Count\n- Protocol_0\n- Protocol_6\n- Protocol_17\n- Flow Packets Per S\n- Fwd Packets Per S\n- Fwd Psh Flags\n- Fwd Act Data Packets\n- Fwd Iat Std\n- Avg Fwd Segment Size\n- Flow Iat Max\n- Total Fwd Packets\n- Subflow Fwd Packets\n- Fwd Iat Min\n- Urg Flag Count\n- Ack Flag Count\n- Rst Flag Count\n- Fwd Packet Length Std\n- Fwd Iat Max\n- Packet Length Min\n- Active Max\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(f\"Total training samples: {len(train_data)}\")\n\nprint(\"Training set label distribution:\\n\", train_data['Label'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:35:10.691678Z","iopub.execute_input":"2025-06-07T03:35:10.692492Z","iopub.status.idle":"2025-06-07T03:35:10.700321Z","shell.execute_reply.started":"2025-06-07T03:35:10.692460Z","shell.execute_reply":"2025-06-07T03:35:10.699395Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total training samples: 2019\nTraining set label distribution:\n Label\n0    1119\n1     900\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"final_feature_names = train_data.drop(columns=['Label']).columns.tolist()\n\nprint(f\"Final number of features used for training: {len(final_feature_names)}\")\nprint(\"Features:\")\nfor feature in final_feature_names:\n    print(f\"- {feature}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:35:17.610972Z","iopub.execute_input":"2025-06-07T03:35:17.611262Z","iopub.status.idle":"2025-06-07T03:35:17.617893Z","shell.execute_reply.started":"2025-06-07T03:35:17.611240Z","shell.execute_reply":"2025-06-07T03:35:17.617160Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Final number of features used for training: 32\nFeatures:\n- Flow Iat Mean\n- Idle Mean\n- Fwd Iat Mean\n- Packet Length Mean\n- Fwd Packet Length Mean\n- Flow Iat Std\n- Fwd Packet Length Min\n- Idle Min\n- Flow Iat Min\n- Init Fwd Win Bytes\n- Packet Length Variance\n- Cwe Flag Count\n- Protocol_0\n- Protocol_6\n- Protocol_17\n- Flow Packets Per S\n- Fwd Packets Per S\n- Fwd Psh Flags\n- Fwd Act Data Packets\n- Fwd Iat Std\n- Avg Fwd Segment Size\n- Flow Iat Max\n- Total Fwd Packets\n- Subflow Fwd Packets\n- Fwd Iat Min\n- Urg Flag Count\n- Ack Flag Count\n- Rst Flag Count\n- Fwd Packet Length Std\n- Fwd Iat Max\n- Packet Length Min\n- Active Max\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Training set class distribution:\")\nprint(y_train.value_counts())\n\nprint(\"Testing set class distribution:\")\nprint(y_test.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:35:30.844488Z","iopub.execute_input":"2025-06-07T03:35:30.845055Z","iopub.status.idle":"2025-06-07T03:35:30.851193Z","shell.execute_reply.started":"2025-06-07T03:35:30.845032Z","shell.execute_reply":"2025-06-07T03:35:30.850132Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training set class distribution:\nLabel\n0    1119\n1     900\nName: count, dtype: int64\nTesting set class distribution:\nLabel\n0    122\n1    115\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"to get statistically significant results, \nhyperparameter tuning is used","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport time\nimport tracemalloc\nfrom sklearn.model_selection import KFold, ParameterGrid\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom pyTsetlinMachine.tm import MultiClassTsetlinMachine\nfrom joblib import Parallel, delayed\n\nX_train_np = np.array(X_train, dtype=np.float32)\ny_train_np = np.array(y_train, dtype=np.int32)\nX_test_np = np.array(X_test, dtype=np.float32)\ny_test_np = np.array(y_test, dtype=np.int32)\n\nparam_grid = {'num_clauses': [200, 500, 1000], 'T': [15, 30, 60], 's': [3.0, 4.0, 6.0]}\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nbest_params, best_f1, lowest_fpr, best_model = None, 0, float('inf'), None\n\ndef train_fold(num_clauses, T, s, X_train_fold, y_train_fold, X_val_fold, y_val_fold):\n    tm = MultiClassTsetlinMachine(num_clauses, T, s)\n    best_epoch_f1, early_stop_counter = 0, 0\n\n    for epoch in range(1, 51):\n        tm.fit(X_train_fold, y_train_fold, epochs=1)\n        y_val_pred = tm.predict(X_val_fold)\n        f1 = f1_score(y_val_fold, y_val_pred, zero_division=0)\n        if f1 > best_epoch_f1:\n            best_epoch_f1, early_stop_counter = f1, 0\n        else:\n            early_stop_counter += 1\n        if early_stop_counter >= 3:\n            break\n\n    y_val_pred = tm.predict(X_val_fold)\n    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_val_pred).ravel()\n    return accuracy_score(y_val_fold, y_val_pred), f1_score(y_val_fold, y_val_pred, zero_division=0), fp / (fp + tn) if (fp + tn) > 0 else 0\n\nfor params in ParameterGrid(param_grid):\n    print(f\"Testing params: {params}\")\n    results = Parallel(n_jobs=-1)(\n        delayed(train_fold)(\n            params['num_clauses'], params['T'], params['s'],\n            X_train_np[train], y_train_np[train],\n            X_train_np[val], y_train_np[val]\n        ) \n        for train, val in kf.split(X_train_np)\n    )\n\n    mean_acc, mean_f1, mean_fpr = np.mean(results, axis=0)\n    print(f\" Params: {params} -> F1: {mean_f1:.4f}, FPR: {mean_fpr:.4f}\")\n\n    if mean_f1 > best_f1 or (mean_f1 == best_f1 and mean_fpr < lowest_fpr):\n        best_f1, lowest_fpr, best_params = mean_f1, mean_fpr, params\n\n        with open(\"/kaggle/working/params.pkl\", \"wb\") as f:\n            pickle.dump(best_params, f)\n        print(\" Best hyperparameters saved.\")\n\nif best_params:\n    print(\"\\n Re-training best model on full training set...\")\n    best_model = MultiClassTsetlinMachine(best_params['num_clauses'], best_params['T'], best_params['s'])\n    best_model.fit(X_train_np, y_train_np, epochs=50)\n\n    with open(\"/kaggle/working/model.pkl\", \"wb\") as model_file:\n        pickle.dump(best_model, model_file)\n    print(\"Trained model saved.\")\n\n    def predict_chunk(model, X_chunk):\n        return model.predict(X_chunk)\n\n    num_jobs = -1\n    num_chunks = max(2, abs(num_jobs) * 2)\n    X_test_chunks = np.array_split(X_test_np, num_chunks)\n\n    tracemalloc.start()\n    start_time = time.time()\n\n    y_test_pred_chunks = Parallel(n_jobs=num_jobs)(\n        delayed(predict_chunk)(best_model, chunk) for chunk in X_test_chunks\n    )\n    y_test_pred = np.concatenate(y_test_pred_chunks)\n\n    inference_time = time.time() - start_time\n    current, peak_memory = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n\n    print(\"\\n Final Test Set Evaluation:\")\n    print(f\"Accuracy  : {accuracy_score(y_test_np, y_test_pred):.4f}\")\n    print(f\"Precision : {precision_score(y_test_np, y_test_pred, zero_division=0):.4f}\")\n    print(f\"Recall    : {recall_score(y_test_np, y_test_pred, zero_division=0):.4f}\")\n    print(f\"F1 Score  : {f1_score(y_test_np, y_test_pred, zero_division=0):.4f}\")\n    print(f\" Inference Time (Parallel): {inference_time:.6f} seconds\")\n    print(f\" Peak Memory during Inference: {peak_memory / 1024:.2f} KB\")\n    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_np, y_test_pred))\n\nelse:\n    print(\" No trained model found. Please check grid search or data.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:48:28.584648Z","iopub.execute_input":"2025-06-07T03:48:28.584958Z","iopub.status.idle":"2025-06-07T03:49:15.884045Z","shell.execute_reply.started":"2025-06-07T03:48:28.584933Z","shell.execute_reply":"2025-06-07T03:49:15.883077Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Testing params: {'T': 15, 'num_clauses': 200, 's': 3.0}\n Params: {'T': 15, 'num_clauses': 200, 's': 3.0} -> F1: 0.5774, FPR: 0.0190\n Best hyperparameters saved.\nTesting params: {'T': 15, 'num_clauses': 200, 's': 4.0}\n Params: {'T': 15, 'num_clauses': 200, 's': 4.0} -> F1: 0.5738, FPR: 0.0177\nTesting params: {'T': 15, 'num_clauses': 200, 's': 6.0}\n Params: {'T': 15, 'num_clauses': 200, 's': 6.0} -> F1: 0.5824, FPR: 0.0389\n Best hyperparameters saved.\nTesting params: {'T': 15, 'num_clauses': 500, 's': 3.0}\n Params: {'T': 15, 'num_clauses': 500, 's': 3.0} -> F1: 0.5503, FPR: 0.0000\nTesting params: {'T': 15, 'num_clauses': 500, 's': 4.0}\n Params: {'T': 15, 'num_clauses': 500, 's': 4.0} -> F1: 0.5492, FPR: 0.0000\nTesting params: {'T': 15, 'num_clauses': 500, 's': 6.0}\n Params: {'T': 15, 'num_clauses': 500, 's': 6.0} -> F1: 0.5525, FPR: 0.0000\nTesting params: {'T': 15, 'num_clauses': 1000, 's': 3.0}\n Params: {'T': 15, 'num_clauses': 1000, 's': 3.0} -> F1: 0.5492, FPR: 0.0000\nTesting params: {'T': 15, 'num_clauses': 1000, 's': 4.0}\n Params: {'T': 15, 'num_clauses': 1000, 's': 4.0} -> F1: 0.5520, FPR: 0.0010\nTesting params: {'T': 15, 'num_clauses': 1000, 's': 6.0}\n Params: {'T': 15, 'num_clauses': 1000, 's': 6.0} -> F1: 0.5752, FPR: 0.0190\nTesting params: {'T': 30, 'num_clauses': 200, 's': 3.0}\n Params: {'T': 30, 'num_clauses': 200, 's': 3.0} -> F1: 0.7304, FPR: 0.2686\n Best hyperparameters saved.\nTesting params: {'T': 30, 'num_clauses': 200, 's': 4.0}\n Params: {'T': 30, 'num_clauses': 200, 's': 4.0} -> F1: 0.7671, FPR: 0.3279\n Best hyperparameters saved.\nTesting params: {'T': 30, 'num_clauses': 200, 's': 6.0}\n Params: {'T': 30, 'num_clauses': 200, 's': 6.0} -> F1: 0.7060, FPR: 0.2489\nTesting params: {'T': 30, 'num_clauses': 500, 's': 3.0}\n Params: {'T': 30, 'num_clauses': 500, 's': 3.0} -> F1: 0.5509, FPR: 0.0010\nTesting params: {'T': 30, 'num_clauses': 500, 's': 4.0}\n Params: {'T': 30, 'num_clauses': 500, 's': 4.0} -> F1: 0.5840, FPR: 0.0720\nTesting params: {'T': 30, 'num_clauses': 500, 's': 6.0}\n Params: {'T': 30, 'num_clauses': 500, 's': 6.0} -> F1: 0.6328, FPR: 0.0668\nTesting params: {'T': 30, 'num_clauses': 1000, 's': 3.0}\n Params: {'T': 30, 'num_clauses': 1000, 's': 3.0} -> F1: 0.5492, FPR: 0.0000\nTesting params: {'T': 30, 'num_clauses': 1000, 's': 4.0}\n Params: {'T': 30, 'num_clauses': 1000, 's': 4.0} -> F1: 0.5531, FPR: 0.0010\nTesting params: {'T': 30, 'num_clauses': 1000, 's': 6.0}\n Params: {'T': 30, 'num_clauses': 1000, 's': 6.0} -> F1: 0.5923, FPR: 0.0287\nTesting params: {'T': 60, 'num_clauses': 200, 's': 3.0}\n Params: {'T': 60, 'num_clauses': 200, 's': 3.0} -> F1: 0.7218, FPR: 0.2084\nTesting params: {'T': 60, 'num_clauses': 200, 's': 4.0}\n Params: {'T': 60, 'num_clauses': 200, 's': 4.0} -> F1: 0.7677, FPR: 0.3310\n Best hyperparameters saved.\nTesting params: {'T': 60, 'num_clauses': 200, 's': 6.0}\n Params: {'T': 60, 'num_clauses': 200, 's': 6.0} -> F1: 0.7998, FPR: 0.3908\n Best hyperparameters saved.\nTesting params: {'T': 60, 'num_clauses': 500, 's': 3.0}\n Params: {'T': 60, 'num_clauses': 500, 's': 3.0} -> F1: 0.6844, FPR: 0.1990\nTesting params: {'T': 60, 'num_clauses': 500, 's': 4.0}\n Params: {'T': 60, 'num_clauses': 500, 's': 4.0} -> F1: 0.6655, FPR: 0.1430\nTesting params: {'T': 60, 'num_clauses': 500, 's': 6.0}\n Params: {'T': 60, 'num_clauses': 500, 's': 6.0} -> F1: 0.6936, FPR: 0.2291\nTesting params: {'T': 60, 'num_clauses': 1000, 's': 3.0}\n Params: {'T': 60, 'num_clauses': 1000, 's': 3.0} -> F1: 0.5525, FPR: 0.0000\nTesting params: {'T': 60, 'num_clauses': 1000, 's': 4.0}\n Params: {'T': 60, 'num_clauses': 1000, 's': 4.0} -> F1: 0.5857, FPR: 0.0363\nTesting params: {'T': 60, 'num_clauses': 1000, 's': 6.0}\n Params: {'T': 60, 'num_clauses': 1000, 's': 6.0} -> F1: 0.7127, FPR: 0.2436\n\n Re-training best model on full training set...\nTrained model saved.\n\n Final Test Set Evaluation:\nAccuracy  : 0.8565\nPrecision : 0.7793\nRecall    : 0.9826\nF1 Score  : 0.8692\n Inference Time (Parallel): 0.013162 seconds\n Peak Memory during Inference: 190.81 KB\n\nConfusion Matrix:\n [[ 90  32]\n [  2 113]]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport time\nimport tracemalloc\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, ParameterGrid\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom joblib import Parallel, delayed\n\nparam_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20], 'min_samples_split': [2, 5]}\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nbest_params, best_f1, lowest_fpr, best_model = None, 0, float('inf'), None\n\ndef train_fold(n_estimators, max_depth, min_samples_split, X_train_fold, y_train_fold, X_val_fold, y_val_fold):\n    model = RandomForestClassifier(\n        n_estimators=n_estimators, max_depth=max_depth,\n        min_samples_split=min_samples_split, random_state=42, n_jobs=-1\n    )\n    model.fit(X_train_fold, y_train_fold)\n\n    y_val_pred = model.predict(X_val_fold)\n    f1 = f1_score(y_val_fold, y_val_pred, zero_division=0)\n    \n    tn, fp, fn, tp = confusion_matrix(y_val_fold, y_val_pred).ravel()\n    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n    return accuracy_score(y_val_fold, y_val_pred), f1, fpr\n\nfor params in ParameterGrid(param_grid):\n    print(f\" Testing params: {params}\")\n    results = Parallel(n_jobs=-1)(\n        delayed(train_fold)(\n            params['n_estimators'], params['max_depth'], params['min_samples_split'],\n            X_train_np[train], y_train_np[train],\n            X_train_np[val], y_train_np[val]\n        )\n        for train, val in kf.split(X_train_np)\n    )\n\n    mean_acc, mean_f1, mean_fpr = np.mean(results, axis=0)\n    print(f\" Params: {params} -> F1: {mean_f1:.4f}, FPR: {mean_fpr:.4f}\")\n\n    if mean_f1 > best_f1 or (mean_f1 == best_f1 and mean_fpr < lowest_fpr):\n        best_f1, lowest_fpr, best_params = mean_f1, mean_fpr, params\n        with open(\"/kaggle/working/best_params_rf.pkl\", \"wb\") as f:\n            pickle.dump(best_params, f)\n        print(\"  Best hyperparameters saved.\")\n\nif best_params:\n    print(f\"\\n Training final model with best params: {best_params}\")\n    best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n    best_model.fit(X_train_np, y_train_np)\n\n    with open(\"/kaggle/working/best_model_rf.pkl\", \"wb\") as model_file:\n        pickle.dump(best_model, model_file)\n    print(\"  Trained model saved.\")\n\n    tracemalloc.start()\n    start_time = time.time()\n\n    y_test_pred = best_model.predict(X_test_np)\n\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n\n    inference_time = end_time - start_time\n    peak_memory_kb = peak / 1024  \n\n    print(\"\\n Test Set Performance (Random Forest):\")\n    print(f\" Accuracy : {accuracy_score(y_test_np, y_test_pred):.4f}\")\n    print(f\" Precision: {precision_score(y_test_np, y_test_pred, zero_division=0):.4f}\")\n    print(f\" Recall   : {recall_score(y_test_np, y_test_pred, zero_division=0):.4f}\")\n    print(f\" F1 Score : {f1_score(y_test_np, y_test_pred, zero_division=0):.4f}\")\n    print(\"\\n Confusion Matrix:\\n\", confusion_matrix(y_test_np, y_test_pred))\n\n    print(f\"\\n Inference Time: {inference_time:.6f} seconds\")\n    print(f\" Peak Memory Usage: {peak_memory_kb:.2f} KB\")\n\nelse:\n    print(\" No trained model found. Please check grid search or data.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T03:45:14.312543Z","iopub.execute_input":"2025-06-07T03:45:14.313239Z","iopub.status.idle":"2025-06-07T03:45:23.411033Z","shell.execute_reply.started":"2025-06-07T03:45:14.313216Z","shell.execute_reply":"2025-06-07T03:45:23.410257Z"}},"outputs":[{"name":"stdout","text":" Testing params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100} -> F1: 0.9586, FPR: 0.0260\n  Best hyperparameters saved.\n Testing params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200} -> F1: 0.9572, FPR: 0.0306\n Testing params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n Params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100} -> F1: 0.9611, FPR: 0.0270\n  Best hyperparameters saved.\n Testing params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n Params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200} -> F1: 0.9572, FPR: 0.0288\n Testing params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100} -> F1: 0.9589, FPR: 0.0287\n Testing params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200} -> F1: 0.9578, FPR: 0.0297\n Testing params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}\n Params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100} -> F1: 0.9577, FPR: 0.0314\n Testing params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n Params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200} -> F1: 0.9560, FPR: 0.0322\n\n Training final model with best params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n  Trained model saved.\n\n Test Set Performance (Random Forest):\n Accuracy : 0.9705\n Precision: 0.9909\n Recall   : 0.9478\n F1 Score : 0.9689\n\n Confusion Matrix:\n [[121   1]\n [  6 109]]\n\n Inference Time: 0.050108 seconds\n Peak Memory Usage: 98.78 KB\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"/kaggle/working/params.pkl\", \"rb\") as file:\n    tm_loaded = pickle.load(file)\n\nprint(\"Model successfully loaded\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:36:14.210862Z","iopub.execute_input":"2025-06-07T03:36:14.211170Z","iopub.status.idle":"2025-06-07T03:36:14.216668Z","shell.execute_reply.started":"2025-06-07T03:36:14.211143Z","shell.execute_reply":"2025-06-07T03:36:14.215841Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model successfully loaded\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Load the trained Tsetlin Machine model\nwith open(\"/kaggle/working/model.pkl\", \"rb\") as file:\n    tm_loaded = pickle.load(file)\n\nprint(\" Model successfully loaded!\")\n\nX_sample = np.array([[0, 1, 0, 1, 1, 0, 1, 0]])  # Replace with actual sample data\n\ny_pred = tm_loaded.predict(X_sample)\n\nprint(f\" Predicted Class: {y_pred[0]}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T03:36:15.760279Z","iopub.execute_input":"2025-06-07T03:36:15.760864Z","iopub.status.idle":"2025-06-07T03:36:15.767792Z","shell.execute_reply.started":"2025-06-07T03:36:15.760836Z","shell.execute_reply":"2025-06-07T03:36:15.766939Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Model successfully loaded!\n Predicted Class: 0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Key takeaways:\n\na) before training, plot correlation matrix to find highly correlated features, and remove redundant features/apply other strategies like PCA or smthg else\n\nb) use binarization strategies as used here, for the features, before training the Tsetlin machine\n\nc) the dataset used here, is useful for academic purposes only, and as an exploratory purpose, to test the waters, so to speak. it is too well-labeled, too detailed, and too separable.\n\nd) Use CPU, not GPU, cuz the Python Package for Tsetlin machine is only supported by CPU","metadata":{}}]}